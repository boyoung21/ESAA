{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWWbnLOupOlVzyAp7DUdCF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boyoung21/ESAA/blob/main/1114_%EB%94%A5%EB%9F%AC%EB%8B%9D_%EC%8B%9C%EC%9E%91.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 인공 신경망의 한계와 딥러닝의 출현\n",
        "\n",
        "오늘날 인공 신경망이 사용하는 입력층, 출력층, 가중치 구조는 퍼셉트론에서 고안됨.\n",
        "\n",
        "퍼셉트론: 다수의 신호를 입력으로 받아 하나의 신호를 출력하는데, 이 신호를 입력으로 받아 흐른다 / 안 흐른다 는 정보를 앞으로 전달하는 원리로 작동\n",
        "\n",
        "컴퓨터가 입력을 논리적으로 인식하는 방식\n",
        "\n",
        "1. AND 게이트: 모든 입력이 1일 때 작동\n",
        "2. OR 게이트: 적어도 한 개가 1일 때, 입력 모두 0만 아니면 1 출력\n",
        "3. XOR 게이트: 둘 중에 1개만 1일 때 작동. 데이터가 비선형적으로 분리돼서 제대로 된 분류가 어려움. 단층 퍼셉트론에서는 학습이 불가능.\n",
        "- 이를 극복하는 방안으로 입출력층 사이에 하나 이상의 중간층, 은닉층을 두어 비선형적으로 분리되는 데이터에 대해서도 학습 가능하도록 다층 퍼셉트론을 고안.\n",
        "- 이렇게 입출력층 사이에 은닉층이 여러 개 있는 신경망을 심층 신경망, DNN이라고 하며, 심층 신경망의 다른 이름을 딥러닝이라고 함.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYq1DLHdfYZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2 딥러닝의 구조\n",
        "\n",
        "딥러닝: 입력층, 출력층과 두 개 이상의 은닉층으로 구성돼 있음.\n",
        "\n",
        "- 입력층: 데이터를 받아들이는 층\n",
        "- 은닉층: 모든 입력 노드로부터 입력 값을 받아 가중합을 계산하고, 이 값을 활성화 함수에 적용하여 출력층에 전달하는 층\n",
        "> 가중치: 노드와 노드 간 연결 정도로, 입력값이 연산 결과에 미치는 영향력을 조절하는 요소\n",
        "\n",
        "  > 바이어스: 가중합에 더해 주는 상수로, 최종 출력값 조절 용도\n",
        "\n",
        "   > 가중합(전달함수): 가중치와 신호의 곱을 합한 것으로, 활성화함수로 전달하는 역할을 하기 때문에 전달함수라고도 함\n",
        "\n",
        "    > 활성화함수: 신호를 입력받아 이를 적절히 처리하여 출력해 주는 함수, 전달함수 값을 적절히 변형\n",
        "    - 시그모이드 함수: 선형 함수의 결과를 0~1 사이에서 비선형 형태로 변형해 줌. 주로 로지스틱과 같은 분류 문제를 확률적으로 표현하는 데 사용. 모델의 깊이가 깊어지면 기울기가 사라지는 기울기 소멸 문제가 발생하여 딥러닝 모델에서는 잘 사용하지 않음.\n",
        "    - 하이퍼볼릭 탄젠트 함수: 선형 함수의 결과를 -1~1 사이에서 비선형 형태로 변환해 줌. 시그모이드에서 결괏값의 평균이 0이 아닌 양수로 편향된 문제를 해결하는 데 사용했으나, 기울기 소멸 문제는 여전함.\n",
        "    - 렐루 함수: 입력이 음수일 때는 0을 출력, 양수일 때는 x 출력. 경사하강법에 영향을 주지 않아 학습 속도가 빠르고 기울기 소멸 x. 입력이 음수일 때는 0을 출력한다는 문제를 해결하기 위해 리키 렐르 함수를 쓰기도 함. 리키렐루 함수는 0이 아닌 0.0001처럼 매우 작은 값을 반환하여 렐루 함수에서 생기는 문제를 해결할 수 있음.\n",
        "- 출력층: 신경망의 최종 결괏값이 포함된 층\n",
        "> 소프트맥스 함수: 입력값을 0~1 사이에 출력되도록 정규화하여 출력 값들의 총합이 1이 되도록 함. 보통 딥러닝에서 출력 노드의 활성화 함수로 많이 사용됨."
      ],
      "metadata": {
        "id": "-5NTzF10hLr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eW52VZ-cfSf4"
      },
      "outputs": [],
      "source": [
        "# 렐루 함수와 소프트맥스 함수를 파이토치에서 구현\n",
        "\n",
        "import torch\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, n_feature, n_hidden, n_output):\n",
        "    super(Net, self).__init__()\n",
        "    self.hidden = torch.nn.Linear(n_feature, n_hidden) # 은닉층\n",
        "    self.relu = torch.nn.ReLU() # 활성화 함수\n",
        "    self.out = torch.nn.Linear(n_hidden, n_output) # 출력층\n",
        "    self.softmax = torch.nn.Softmax(dim = n_output) # 소프트맥스 함수\n",
        "  def forward(self, x):\n",
        "    x = self.hidden(x)\n",
        "    x = self.relu(x) # 은닉층을 위한 렐루 활성화 함수\n",
        "    x = self.out(x)\n",
        "    x = self.softmax(x) # 출력층을 위한 소프트맥스 활성화 함수\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 손실 함수\n",
        "\n",
        "경사하강법: 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트하는 방법. 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동시키는 방법. 이때 오차를 구하는 방법이 손실 함수.\n",
        "\n",
        "손실 함수: 학습을 통해 얻은 데이터의 추정치가 실제 데이터와 얼마나 차이가 나는지 평가하는 지표.\n",
        "- 회귀일 때는 MSE, 분류에서 원핫인코딩 했을 때는 크로스 엔트로피\n",
        "- 일반적으로 MSE와 시그모이드 활성화함수를 결합하여 사용하면 시그모이드 특성으로 기울기가 매끄럽지 못하고 학습 속도도 느림. 이를 극복하기 위해 고안된 방법이 크로스 엔트로피로, 두 개의 확률 분포 차이를 이용하기 때문에 시그모이드 영향을 덜 받음."
      ],
      "metadata": {
        "id": "VVyJdYAvkmKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
        "y_pred = model(x)\n",
        "loss = loss_fn(y_pred, y)"
      ],
      "metadata": {
        "id": "iTWb1JbHlnGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.CrossEntropyLoss()\n",
        "input = torch.randn(5, 6, requires_grad=True) # torch.randn은 평균이 0이고 표준편차가 1인 가우시안 정규분포를 이용하여 숫자를 생성\n",
        "target = torch.empty(3, dtype=torch.long).random_(5) # torch.empty는 dtype torch.float32의 랜덤한 값으로 채워진 텐서를 반환\n",
        "output = loss(input, target)\n",
        "output.backward()"
      ],
      "metadata": {
        "id": "ydJkJHo2hLXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2.2 딥러닝 학습\n",
        "\n",
        "1. 순전파: 네트워크, 즉 모델에 훈련 데이터가 들어올 때 발생. 모든 뉴런이 이전 층의 뉴런에서 수신한 정보에 변환을 적용하여 다음 층의 뉴런으로 전송하는 방식.\n",
        "\n",
        "2. 역전파: 순전파 과정으로 예측 값이 최종 층에 도달하면, 네트워크의 예측값과 실제값 차이를 추정 후 이 정보가 출력층 -> 은닉층 -> 입력층으로 역으로 전파됨. 오차를 각 뉴런의 가중치로 미분한 후 기존 가중치 값에서 빼서 업데이트하고 이를 다시 순전파의 가중치 값으로 이용하는 것이 역전파 과정\n",
        "\n"
      ],
      "metadata": {
        "id": "O5Bp-1wImRrd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2.3 딥러닝의 문제점과 해결 방안\n",
        "\n",
        "딥러닝은 활성화 함수가 적용된 여러 은닉층을 결합하여 비선형 영역을 표현하는 것으로, 은닉층 개수 많을수록 데이터 분류 잘됨. 다만 몇 가지 고려할 점 존재\n",
        "\n",
        "1. 과적합 발생 가능성이 큼: 이를 방지하기 위해 드롭아웃 방법 존재. 학습 과정 중에 임의로 일부 노드들을 학습에서 제외함.\n",
        "\n",
        "2. 기울기 소멸 문제: 주로 렐루 함수를 활성화함수로 선택하여 해결.\n",
        "\n",
        "3. 성능 나빠지는 문제 발생: 확률적 경사 하강법과 배치경사하강법 사용\n",
        "- 확률적 경사 하강법: 임의로 선택한 데이터에 대해 기울기 계산하는 방법으로, 속도가 빠르나 파라미터 변경 폭이 불안정\n",
        "- 배치경사하강법: 전체 데이터셋에 대한 오류를 구한 후 기울기를 한 번만 계산하여 모델 파라미터 업데이트. 모든 데이터를 사용해서 오래 걸린다는 단점 존재.\n",
        "- 미니배치: 각 배치마다의 평균 기울기 이용해서 모델 업데이트. 제일 안정적이라 널리 사용됨."
      ],
      "metadata": {
        "id": "b2p-sWksnEku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드롭아웃 예시 코드\n",
        "\n",
        "class DropoutModel(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(DropoutModel, self).__init__()\n",
        "    self.layer1 = torch.nn.Linear(784, 1200)\n",
        "    self.dropout1 = torch.nn.Dropout(0.5) # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미\n",
        "    self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "    self.dropout2 = torch.nn.Dropout(0.5)\n",
        "    self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.layer1(x))\n",
        "    x = self.dropout1(x)\n",
        "    x = F.relu(self.layer2(x))\n",
        "    x = self.dropout2(x)\n",
        "    return self.layer3(x)"
      ],
      "metadata": {
        "id": "frnAjKjSn5S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파이토치 확률적경사하강법 구현 예시 코드\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.x_data = [[1,2,3],[4,5,6],[7,8,9]]\n",
        "    self.y_data = [[12],[18],[11]]\n",
        "    def __len__(self):\n",
        "      return len(self.x_data)\n",
        "    def __getitem__(self, idx):\n",
        "      x = torch.FloatTensor(self.x_data[idx])\n",
        "      y = torch.FloatTensor(self.y_data[idx])\n",
        "      return x, y\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(\n",
        "    dataset, # 데이터셋\n",
        "    batch_size = 2, # 미니 배치 크기로 2의 제곱수를 사용하겠다는 의미\n",
        "    shuffle = True # 데이터를 불러올 때마다 랜덤으로 섞여서 가져옴\n",
        "    )"
      ],
      "metadata": {
        "id": "BpDxmBJiooY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 옵티마이저\n",
        "\n",
        "확률적 경사 하강법의 파라미터 변경 폭이 불안정한 문제를 해결하기 위해 학습 속도와 운동량을 조정하는 옵티마이저\n",
        "\n",
        "1. 속도를 조정하는 방법\n",
        "- 아다그라드: 변수(가중치)의 업데이트 횟수에 따라 학습률 조정하는 방법. 많이 변화하는 변수들의 학습률은 작게 하고, 많이 변화하지 않는 변수들의 학습률은 크게 함.\n",
        "- 아다델타: 아다그라드에서 기울기 크기 누적 값이 커짐에 따라 학습을 멈추는 문제를 방지하기 위해 등장. 아다그라드 수식에서 학습률을 가중치의 변화량 크기를 누적한 값으로 변환했기 때문에 학습률에 대한 하이퍼파라미터가 필요하지 않음.\n",
        "- 알엠에스프롭: 아다그라드의 기울기 크기 누적 값이 무한히 커지는 것을 방지하고자 등장. 학습이 안 되는 문제를 해결하기 위해 기울기 크기 누적 함수에서 gamma만 추가됨. 기울기 누적이 너무 크면 학습률이 작아져서 학습이 안 될 수 있으니, 이때 감마값을 이용하여 학습률 크기를 비율로 조정할 수 있도록\n",
        "\n",
        "2. 운동량 조절\n",
        "- 모멘텀: 경사하강법처럼 매번 기울기를 구하지만 가중치 수정 전에 이전 수정 방향을 참고하여 같은 방향으로 일정한 비율만 수정하는 방법. 수정이 양이나 음의 방향 지그재그로 발생하는 현상이 줄고 관성 효과를 얻을 수 있으며, SGD랑 함께 사용\n",
        "- 네스테로프 모멘텀: 관성에 의해 멈춰야 할 시점에서도 멀리 가 버리는 모멘텀의 단점을 보완. 모멘텀으로 절반 정도 이동 후에 어떤 방식으로 이동할지를 다시 계산하여 결정. 모멘텀의 빠른 속도 이점과 더불어 제동 효과.\n",
        "\n",
        "3. 속도와 운동량 혼용: 아담\n",
        "- 모멘텀과 알엠에스프롭 장점을 결합한 경사하강법\n",
        "\n"
      ],
      "metadata": {
        "id": "VYfg4ws9ohlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.2.4 딥러닝을 사용할 때 이점\n",
        "\n",
        "1. 특성 추출: 데이터별 특징 찾아서 이것을 토대로 데이터를 벡터로 변환하는 작업. 은닉층을 깊게 쌓은 덕분에 도메인 지식 없이도 데이터 특성 잘 잡아낼 수 있게 됨.\n",
        "\n",
        "2. 빅데이터 충분한 활용: 데이터 사례 많을수록 성능 향상. 데이터 작으면 머신러닝 추천\n",
        "\n"
      ],
      "metadata": {
        "id": "kD-hwEzTvvK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.3 딥러닝 알고리즘\n",
        "\n",
        "1. 합성곱 신경망 CNN: 합성곱층과 풀링층이 내재된 것으로, 이미지 처리 시 좋은 성능을 내는 신경망. 객체, 얼굴, 장면 인식하는 패턴 찾을 때 유용. 기존 신경망과 달리 각 층의 입출력을 항상 유지하며 복수 필터로 이미지 특징 추출하고 학습. 이 추출하고 학습한 특징을 풀링층을 통해 특징 강화. 또한, 학습 파라미터가 매우 적은 것도 장점.\n",
        "\n",
        "2. 순환 신경망 RNN: 시계열 데이터처럼 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공신경망. 결과끼리의 연쇄가 존재하는데, 현재 결과는 이전 결과와 연관 있음. 기울기 소멸 문제로 학습이 제대로 안 되는 경우를 보완한 LSTM도 존재.\n",
        "\n",
        "3. 제한된 볼츠만 머신: 기시층과 은닉층으로 구성된 모델. 가시층은 은닉층과만 연결되는데, 이게 제한된 볼츠만 머신. 차원감소, 분류, 선형회귀분석, 협업 필터링 등 기울기 소멸 문제 해결 위해 사전 학습 용도로 활용 가능. 심층 신뢰 연결망의 요소로 많이 사용됨.\n",
        "\n",
        "4. 심층 신뢰 신경망: 제한된 볼츠만 머신을 블록처럼 여러 층으로 쌓은 형태로 연결된 신경망. 비지도 학습에 잘 쓰이며 부분적 이미지에서 전체를 연상하는 일반화와 추성화 과정 구현 때 사용하면 유용함."
      ],
      "metadata": {
        "id": "oog2VJQUwEog"
      }
    }
  ]
}